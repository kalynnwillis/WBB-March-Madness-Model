---
title: "Women's Basketball March Madness: Bradley-Terry Model Analysis"
subtitle: "Analyzing Mid-Tier Seed (8-12) Advancement Probabilities"
author: "Kalynn Willis, Jasmine Peck, Ellie Brothers"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    code_folding: hide
    fig_width: 10
    fig_height: 6
  pdf_document:
    toc: true
    toc_depth: 3
    fig_width: 10
    fig_height: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    fig.align = "center"
)

library(tidyverse)
library(here)
library(knitr)
library(kableExtra)

read_if_exists <- function(path) {
    if (file.exists(path)) {
        if (grepl("\\.rds$", path)) {
            return(readRDS(path))
        } else if (grepl("\\.csv$", path)) {
            return(read_csv(path, show_col_types = FALSE))
        }
    }
    return(NULL)
}
```

```{r load-results, echo=FALSE}
key_results <- read_if_exists(here("results", "tables", "key_results.rds"))
sweet16_probs <- read_if_exists(here("results", "tables", "sweet16_mid_tier_probabilities.csv"))
```

# Executive Summary 

The NCAA Women’s Basketball Tournament takes place in March, which makes it considered March Madness. While generally, people assume that the mid- to high-tier teams make it to the “Sweet 16” (Top 16 teams in the tournament), in 2024, that did not happen, as no mid-tier teams made it to the Top 16. The question we want to answer is if a team ranked 8-12 advances to the second round, what is the chance they make it to the finals (or even win)?

The data we used was from the wehoop R package, which provides play-by-play data, box scores, and game results from 2002-2024 for WNBA and WNCAA (and we will use the WNCAA data). Our analysis consisted of over 5,000 season games to create strength ratings for all teams in the tournament by evaluating their opponents and performance rather than wins and losses. Then, we simulated the tournaments 5,000 times to see outcomes from teams seeded 8-12. 

Some of our limitations are notable, being that the data also does not provide us with the observed Women’s NCAA Tournament seeds, so we create our seeds based on estimated team strengths. Real seeds take into consideration committee choices, schedule strength, location, quality wins/losses, etc. Additionally, we do not take into account the changes across seasons, whether that be roster changes or different coaches. 

Our methodology consisted of using a Bradley-Terry Model, which helped us estimate team strength to determine seed placements for our tournament simulations. The win probability between teams depends on the difference in team strengths, indicating in our model whether teams played at home, and testing for alternative controlling variables that might affect the outcome. There is a large variability for team strength in our ranked seed levels for teams across seasons, since our model takes the average advancement of seeds per season across the 5 seasons. Lastly, we simulated 5,000 tournaments and tracked the 8-12 seed advancement from the Sweet 16 to the championship, where the distribution of teams narrowed as we further progressed in the tournament.

Overall, our results display that 2.21 teams in seeds 8-12 advance to the Sweet 16, 45% of those same teams reach the Elite 8, 15.3% reach the Final 4, and 1.9% of those same teams will win the championship. of those same teams. In reference to the 2023-2024 season, no mid-tier teams made it to the Top 16, which has about a 6.8% probability of occurring, which is very unusual. Therefore, fans can celebrate when the team they cheer for is seeded 10 and advances to the Elite 8.

---

# Introduction

## Motivation and Problem Statement

The NCAA Women’s Basketball tournament, often called March Madness, includes 64 teams and engages viewers across the United States each year through March Madness bracket creation. Individuals take educated guesses as to which teams will win each game and eventually the entire tournament, and because of the single-elimination format, these unpredictable games with frequent upsets are very engaging. In each of the four regions, 16 teams are seeded based on their rankings. Understanding the probability of upsets, which occurs when a lower seeded team wins against a higher-seeded team, is valuable for bracket predictions, how competitive the two teams are, and identifying over or under seeded teams.

In theory, if every seed was correct with 100% accuracy, no team seeded higher than a 4 seed would make it to the Sweet 16. But in reality, we know upsets occur – for example, consider the Duke (#7 seed) versus Ohio State (#2 seed) Women’s March Madness second round game in 2024. Unexpectedly, Duke came out on top in a 75-63 point victory, their advancement to the Sweet 16. Many individuals would have had Ohio State advancing in their brackets. In addition, it makes us wonder if Duke and Ohio State were more competitive than the average game between a 7 and 2 seed, so it would be valuable to know the probability that Duke would win (or any other underdog) for the future.

## Research Questions

We were curious to take a more in-depth look at teams lying within the bottom half of the rankings, but still strong enough to potentially make an impact on the March Madness brackets (seed 8 – 12 in all 4 regions). Specifically, we ask how many of the 8 - 12 seeds should be expected to advance to the Sweet 16 and if they advance past the second round, what the chance is that they make it all the way to the final. This is interesting because they are not automatic first-round losers or overwhelming favorites – individuals making bracket predictions may want to place bets in the 8 - 12 seeds if the probabilities of them winning are high enough to take the underdog. In both the 2023 and 2024 Women’s March Madness seasons, there wasn’t a single team seeded 8 - 12 that advanced to the Sweet 16, so we are curious how often we should expect this result.

---

# Methodology

## Data Collection and Preparation

```{r data-summary, echo=FALSE}
# Load data info
team_abilities <- read_if_exists(here("data", "processed", "team_abilities_with_seeds.rds"))
games_cleaned <- read_if_exists(here("data", "processed", "games_cleaned.rds"))

if (!is.null(team_abilities) && !is.null(games_cleaned)) {
    n_teams <- nrow(team_abilities)
    n_games <- nrow(games_cleaned)
    n_tournament_teams <- sum(!is.na(team_abilities$seed))
    n_mid_tier <- sum(team_abilities$seed >= 8 & team_abilities$seed <= 12, na.rm = TRUE)
} else {
    n_teams <- NA
    n_games <- NA
    n_tournament_teams <- NA
    n_mid_tier <- NA
}
```

### Data Source and Collection

We collected our data using the **wehoop** R package (which includes comprehensive game-level data from ESPN) and pulled Women's NCAA Basketball data for 5 seasons: 2019 and 2021-2024. We chose to exclude 2020 due to potential disruptions from COVID-19. In total, we analyzed **`r n_teams` teams** with **`r format(n_games, big.mark=",")` total games**. Our final tournament bracket included 64 teams with **`r n_mid_tier` teams seeded 8-12**.

```{r data-collection-code, eval=FALSE}
# From scripts/01_data_collection_UPDATED.R
suppressPackageStartupMessages({
    library(tidyverse)
    library(lubridate)
    library(here)
})

SEASONS <- c(2019, 2021, 2022, 2023, 2024)
set.seed(479)

if (!requireNamespace("wehoop", quietly = TRUE)) {
    install.packages("wehoop")
}
library(wehoop)

pick_first <- function(cands, nm) {
    hit <- intersect(cands, nm)
    if (length(hit)) hit[1] else NA_character_
}

raw <- wehoop::load_wbb_schedule(seasons = SEASONS)
stopifnot(is.data.frame(raw), nrow(raw) > 0)

nm <- names(raw)
cat("Columns returned by wehoop:\n")
print(nm)

date_col <- pick_first(c("game_date", "start_date", "date"), nm)
season_col <- pick_first(c("season", "season_year"), nm)
id_col <- pick_first(c("game_id", "id", "espn_game_id"), nm)

home_team_col <- pick_first(c(
    "home_display_name", "home_name", "home_location", "home_short_display_name",
    "home_team", "home_team_name", "home_team_display_name", "home_team_short_display_name"
), nm)

away_team_col <- pick_first(c(
    "away_display_name", "away_name", "away_location", "away_short_display_name",
    "away_team", "away_team_name", "away_team_display_name", "away_team_short_display_name"
), nm)

home_score_col <- pick_first(c("home_score", "home_points", "home_team_score"), nm)
away_score_col <- pick_first(c("away_score", "away_points", "away_team_score"), nm)

neutral_col <- pick_first(c("neutral_site", "neutral", "is_neutral_site", "site_neutral"), nm)
type_col <- pick_first(c("season_type", "game_type", "tournament_type", "conference_competition"), nm)

must_have <- c(date_col, season_col, home_team_col, away_team_col)
if (any(is.na(must_have))) {
    stop("Critical columns missing after remapping.")
}

games_combined <- raw |>
    transmute(
        game_id = if (!is.na(id_col)) as.character(.data[[id_col]]) else NA_character_,
        season = suppressWarnings(as.integer(.data[[season_col]])),
        game_date = suppressWarnings(lubridate::ymd(.data[[date_col]])),
        home_team = as.character(.data[[home_team_col]]),
        away_team = as.character(.data[[away_team_col]]),
        home_score = if (!is.na(home_score_col)) suppressWarnings(as.integer(.data[[home_score_col]])) else NA_integer_,
        away_score = if (!is.na(away_score_col)) suppressWarnings(as.integer(.data[[away_score_col]])) else NA_integer_,
        neutral_site = if (!is.na(neutral_col)) {
            val <- .data[[neutral_col]]
            as.integer(val %in% c(TRUE, 1, "1", "TRUE", "True", "true", "Neutral", "NEUTRAL"))
        } else {
            0L
        },
        season_type = if (!is.na(type_col)) .data[[type_col]] else NA,
        game_type_raw = if (!is.na(type_col)) as.character(.data[[type_col]]) else NA_character_
    ) |>
    distinct(game_id, .keep_all = TRUE)

# D-I NORMALIZATION
normalize_name <- function(x) stringr::str_squish(stringr::str_to_lower(x))

n_games_before_d1 <- nrow(games_combined)

d1_teams <- tryCatch(
    suppressMessages(purrr::map_dfr(SEASONS, wehoop::espn_wbb_teams)),
    error = function(e) NULL
)

if (is.null(d1_teams) || !"team_id" %in% names(d1_teams)) {
    games_norm <- games_combined
} else {
    nm_t <- names(d1_teams)
    name_col <- pick_first(c("display_name", "short_display_name", "name", "team", "school"), nm_t)
    class_col <- pick_first(c("classification", "org", "org_type"), nm_t)
    div_col <- pick_first(c("division", "division_name", "division_short"), nm_t)

    wbb_di <- d1_teams |>
        dplyr::mutate(
            .team_name = if (!is.na(name_col)) .data[[name_col]] else NA_character_,
            .class     = if (!is.na(class_col)) .data[[class_col]] else NA_character_,
            .div       = if (!is.na(div_col)) .data[[div_col]] else NA_character_
        ) |>
        dplyr::filter(
            is.na(.class) | stringr::str_detect(tolower(.class), "ncaa"),
            is.na(.div) | stringr::str_detect(tolower(.div), "^(1|i|division i)\\b")
        ) |>
        dplyr::transmute(
            di_team = .team_name,
            di_key = normalize_name(.team_name)
        ) |>
        dplyr::distinct() |>
        dplyr::filter(!is.na(di_key), di_key != "")

    games_norm <- games_combined |>
        dplyr::mutate(
            home_key = normalize_name(home_team),
            away_key = normalize_name(away_team)
        ) |>
        dplyr::left_join(wbb_di, by = c("home_key" = "di_key")) |>
        dplyr::rename(home_di_team = di_team) |>
        dplyr::left_join(wbb_di, by = c("away_key" = "di_key")) |>
        dplyr::rename(away_di_team = di_team) |>
        dplyr::filter(!is.na(home_di_team), !is.na(away_di_team)) |>
        dplyr::mutate(
            home_team = home_di_team,
            away_team = away_di_team
        ) |>
        dplyr::select(-home_di_team, -away_di_team, -home_key, -away_key)
}

# REGULAR-SEASON FILTER
stype <- suppressWarnings(as.integer(games_norm$season_type))

if (!all(is.na(stype)) && any(stype == 2, na.rm = TRUE)) {
    games_cleaned <- games_norm |> filter(stype == 2)
} else {
    stxt <- tolower(as.character(games_norm$season_type))
    stxt[is.na(stxt)] <- ""
    games_cleaned <- games_norm |>
        filter(
            stringr::str_detect(stxt, "regular|season\\b") |
                (!stringr::str_detect(stxt, "post|tourn|conf|champ|playoff") &
                    lubridate::month(game_date) <= 2)
        )
}

# Completed games with valid scores, no ties
games_cleaned <- games_cleaned |>
    filter(!is.na(home_score), !is.na(away_score)) |>
    filter(home_score >= 0, away_score >= 0) |>
    filter(!is.na(game_date)) |>
    filter(home_score != away_score) |>
    mutate(
        home_winner = as.integer(home_score > away_score),
        away_winner = 1L - home_winner
    )

if (nrow(games_cleaned) == 0) stop("No regular-season completed games after filtering.")

if (!requireNamespace("igraph", quietly = TRUE)) install.packages("igraph")
library(igraph)

g <- graph_from_data_frame(
    games_cleaned |> distinct(home_team, away_team),
    directed = FALSE
)
comps <- components(g)
largest_comp <- which.max(comps$csize)
teams_in_cc <- names(comps$membership[comps$membership == largest_comp])

games_cleaned <- games_cleaned |>
    filter(home_team %in% teams_in_cc, away_team %in% teams_in_cc)

# Minimum games filter
team_game_counts <- bind_rows(
    games_cleaned |> count(team = home_team, name = "n"),
    games_cleaned |> count(team = away_team, name = "n")
) |>
    group_by(team) |>
    summarise(total_games = sum(n), .groups = "drop")

eligible_teams <- team_game_counts |>
    filter(total_games >= 8) |>
    pull(team)

games_cleaned <- games_cleaned |>
    filter(home_team %in% eligible_teams, away_team %in% eligible_teams)

# Save data
dir.create(here("data", "raw"), recursive = TRUE, showWarnings = FALSE)
dir.create(here("data", "processed"), recursive = TRUE, showWarnings = FALSE)

readr::write_csv(games_combined, here("data", "raw", "games_raw.csv"))
saveRDS(games_combined, here("data", "raw", "games_raw.rds"))

readr::write_csv(games_cleaned, here("data", "processed", "games_cleaned.csv"))
saveRDS(games_cleaned, here("data", "processed", "games_cleaned.rds"))
```

### Data Filtering and Quality Control

We applied D-I team normalization before any filtering, then filtered for data quality:

- Regular season games only: Excluded exhibition games, conference tournaments, and NCAA tournament games
- Complete games only: Required non-missing scores and valid game dates
- No ties: Removed games ending in ties
- Connected component: Kept only teams in the largest connected network (all teams must be transitively comparable)
- Minimum games: Teams must have played at least 8 games

### Bradley-Terry Data Preparation

We prepared the data structure for Bradley-Terry model fitting by aggregating games into pairwise matchup counts. For each pair of teams that played each other, we counted home wins, away wins, and calculated the proportion of home-advantage games (non-neutral site).

```{r bt-data-prep-code, eval=FALSE}
# From scripts/01_data_collection_UPDATED.R (continued)

# Build Bradley-Terry pair table
unique_teams <- sort(unique(c(games_cleaned$home_team, games_cleaned$away_team)))

bt_data <- games_cleaned |>
    rename(home.team = home_team, away.team = away_team) |>
    group_by(season, home.team, away.team) |>
    summarise(
        home.wins = sum(home_winner),
        away.wins = sum(away_winner),
        total_games = dplyr::n(),
        home_adv_bar = mean(1L - neutral_site),
        .groups = "drop"
    ) |>
    mutate(
        home.team = factor(home.team, levels = unique_teams),
        away.team = factor(away.team, levels = unique_teams)
    )

# TOURNAMENT SEEDS: Real NCAA Data vs Model-Based
ncaa_seed_file <- here("data", "raw", "ncaa_seeds_historical.csv")
    
    tournament_seeds <- read_csv(ncaa_seed_file, show_col_types = FALSE) |>
        filter(season == max(SEASONS)) |>
        select(team, seed = ncaa_seed, region) |>
        mutate(
            team = str_trim(team),
            season = max(SEASONS)
        )
    
    available_teams <- unique(c(games_cleaned$home_team, games_cleaned$away_team))
    
    tournament_seeds <- tournament_seeds |>
        filter(team %in% available_teams)
} else {
    # Model-based seeding using win percentage
    team_perf <- games_cleaned |>
        transmute(team = home_team, win = home_winner, season = season) |>
        bind_rows(games_cleaned |>
            transmute(team = away_team, win = away_winner, season = season)) |>
        group_by(team) |>
        summarise(
            games = n(),
            wins = sum(win),
            win_pct = wins / games,
            n_seasons = n_distinct(season),
            .groups = "drop"
        ) |>
        arrange(desc(win_pct)) |>
        slice_head(n = 64)
    
    tournament_seeds <- team_perf |>
        mutate(
            seed = rep(1:16, length.out = n()),
            region = rep(c("Portland", "Albany", "Spokane", "Wichita"), length.out = n()),
            season = max(SEASONS)
        ) |>
        select(season, team, seed, region)
}

mid_tier_seeds <- tournament_seeds |>
    filter(seed >= 8, seed <= 12) |>
    mutate(seed_category = "8-12 seeds")

# Save Bradley-Terry data and tournament seeds
readr::write_csv(bt_data, here("data", "processed", "bt_data.csv"))
saveRDS(bt_data, here("data", "processed", "bt_data.rds"))

readr::write_csv(tournament_seeds, here("data", "processed", "tournament_seeds.csv"))
saveRDS(tournament_seeds, here("data", "processed", "tournament_seeds.rds"))

readr::write_csv(mid_tier_seeds, here("data", "processed", "mid_tier_seeds.csv"))
saveRDS(mid_tier_seeds, here("data", "processed", "mid_tier_seeds.rds"))
```

This creates a pairwise comparison dataset suitable for Bradley-Terry modeling.

---

## Bradley-Terry Model Framework

### Model Specification

We use the Bradley-Terry model as a probabilistic framework that models pairwise comparisons—in this case, game outcomes between women's NCAA basketball teams. For teams $i$ and $j$ with latent strength parameters $\lambda_i$ and $\lambda_j$, the probability that Team $i$ beats Team $j$ is:

$$
P(\text{team } i \text{ beats team } j) = \frac{e^{\lambda_i}}{e^{\lambda_i} + e^{\lambda_j}} = \frac{1}{1 + e^{-(\lambda_i - \lambda_j)}}
$$

The parameter lambda represents the log-strength of Team $i$. The larger the lambda, the stronger the team. Win probability depends on the **difference** of lambda values: the larger the difference, the more probable it is that the stronger team will win.

### Synthetic Tournament Seeds

For our analysis, we used synthetic tournament seeds based on team strength generated by Bradley-Terry lambda values. We do not analyze historical Women's March Madness brackets; instead, we use regular season games to predict how teams seeded 8-12 would perform in the tournament.

We assigned seeds 1-16 to the top 64 teams based on their Bradley-Terry latent strength parameters across the five seasons. This approach excludes actual NCAA committee-selected seeds, which can be affected by factors including geographic restrictions, conference tournament results, and subjective decisions. 

Synthetic seeding allows for an unbiased analysis demonstrating tournament results under "perfect seeding"—where seeds perfectly reflect team strength as measured by regular season performance.

### Covariates and Model Comparison

Our base model includes a home team indicator variable to control for home-court advantage. We explored additional covariates to enhance the Bradley-Terry model by capturing different dimensions of team performance beyond simple win-loss records.

**Rationale for Covariate Selection:**

Traditional Bradley-Terry models use only game outcomes (win/loss), but basketball performance has multiple measurable dimensions. We considered four types of performance metrics commonly used in basketball analytics:

1. Offensive Rating (off_diff): Points scored per game. Teams with strong offenses may have different tournament trajectories than defensive-minded teams, even with identical records.

2. Defensive Rating (def_diff): Points allowed per game. Strong defensive teams might perform better under tournament pressure when every possession matters.

3. Net Rating (net_diff): Point differential per game (offensive rating minus defensive rating). This composite metric captures overall dominance—teams that consistently win by large margins may be stronger than teams that win close games.

4. Margin of Victory (mov_diff): Average winning margin in games won. Unlike net rating (which includes losses), MOV focuses specifically on how dominant teams are in their victories, potentially identifying teams that "turn it on" when ahead.

We hypothesized that incorporating these metrics might improve predictions because:
- Two teams with identical 20-5 records may differ significantly in dominance (winning by 2 vs. 20 points on average)
- Tournament games often involve different competitive dynamics than close regular-season games
- Performance metrics might capture team qualities (offensive firepower, defensive consistency) not fully reflected in win-loss records

**Models Evaluated:**

We fit five models and compared their predictive performance on a holdout test set (2024 season), training on 2019, 2021-2023:

1. Base Model: `team + home_advantage`
2. Net Rating Model: `team + home_advantage + net_diff` (overall dominance)
3. Off/Def Model: `team + home_advantage + off_diff + def_diff` (separate offensive/defensive effects)
4. MOV Model: `team + home_advantage + mov_diff` (winning margin in victories)
5. Combined Model: `team + home_advantage + net_diff + mov_diff` (overall dominance + victory margins)

**Results:**

| Model | Test Accuracy | Log-Loss | Brier Score | Calibration MSE |
|:------|:-------------:|:--------:|:-----------:|:---------------:|
| Base | **69.89%** | **0.5779** | **0.1970** | **0.0092** |
| Net Rating | 69.89% | 0.5779 | 0.1970 | 0.0092 |
| Off/Def | 69.89% | 0.5779 | 0.1970 | 0.0092 |
| MOV | 69.89% | 0.5779 | 0.1970 | 0.0092 |
| Combined | 69.89% | 0.5779 | 0.1970 | 0.0092 |

We compared the predictive performance of multiple models rather than relying solely on AIC. We utilized multiple model comparisons including Log-Loss, Brier Score, and Calibration MSE. The model comparison tests all generated the same results (Log-Loss = 0.5779, Brier Score = 0.1970, Calibration MSE = 0.0092), indicating redundancy in the covariates. Given that the covariates did not return more accurate predictions than our base model, we proceed with the base Bradley-Terry model for simplicity throughout our analysis. 

```{r lambda-comparison-code, eval=FALSE}
# From scripts/09_model_comparison.R
team_abilities_base <- readRDS(here("data", "processed", "team_abilities_with_seeds.rds"))
team_abilities_cov <- readRDS(here("data", "processed", "team_abilities_with_seeds_cov.rds"))

comparison <- team_abilities_base %>%
    select(team, lambda_base = lambda, seed) %>%
    inner_join(
        team_abilities_cov %>% select(team, lambda_cov = lambda),
        by = "team"
    )

cor_value <- cor(comparison$lambda_base, comparison$lambda_cov, use = "complete.obs")

p <- ggplot(comparison, aes(x = lambda_base, y = lambda_cov)) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray40") +
    geom_point(alpha = 0.5, color = "#0072B2") +
    annotate("text", x = min(comparison$lambda_base, na.rm = TRUE) + 0.5, 
             y = max(comparison$lambda_cov, na.rm = TRUE) - 0.5,
             label = sprintf("r = %.3f", cor_value), 
             size = 5, fontface = "bold") +
    coord_fixed() +
    labs(
        title = "Team Abilities: Base Model vs. Covariate Model",
        subtitle = "High correlation → covariates add no predictive value",
        x = "Lambda (Base Model: Team + Home)",
        y = "Lambda (Covariate Model: Team + Home + Covariates)"
    ) +
    theme_minimal(base_size = 12) +
    theme(plot.title = element_text(face = "bold"))

ggsave(here("results", "figures", "18_lambda_comparison.png"),
       p, width = 8, height = 8, dpi = 300)
```

```{r lambda-comparison, echo=FALSE, fig.cap="Team abilities (lambda) from base model vs. covariate model show strong correlation (r = 0.85), but predictions are identical because relative team differences remain unchanged."}
knitr::include_graphics(here("results", "figures", "18_lambda_comparison.png"))
```

**Why are covariates redundant?**

The covariates are redundant because the team strength (λ) correlates strongly with the performance metrics. For example, teams with a higher offensive rating that score more points, will win more games, which increases their generated λ. Implementing these covariates into the model doesn't add new information. 

**Model Selection Justification:**

The Bradley-Terry model measuring the team lambda and home advantage is the optimal choice for our model. This is because it is simpler and thus more interpretable. Because there are fewer parameters, this base model will reduce overfitting risk compared to the other models we generated.

In general, the Bradley-Terry model has many benefits when analyzing the March Madness bracket including generating results even though not all teams play each other. Also, the probabilistic predictions will allow explanations of which team is stronger through generating win probabilities for each matchup.

### Model Fitting

We create the Bradley-Terry model using maximum likelihood estimation to create a log-likelihood model for each game. The log-likelihood for $n$ games is:

$$
\ell(\lambda) = \sum_{k=1}^{n} \left[ \lambda_{i_k} - \log(e^{\lambda_{i_k}} + e^{\lambda_{j_k}}) \right]
$$

where game $k$ was won by team $i_k$ over team $j_k$.

We fit separate Bradley-Terry models for each season because each seasonal tournament has different teams at different seeds. After fitting these models, we aggregate our results using inverse-variance weighting – which means we give more weight to the results that are more precise. 

```{r bt-model-fitting-code, eval=FALSE}
# From scripts/02_bradley_terry_model.R

library(tidyverse)
library(here)

if (!require("BradleyTerry2")) {
    install.packages("BradleyTerry2")
    library(BradleyTerry2)
}

# Load data
bt_data <- readRDS(here("data", "processed", "bt_data.rds"))
tournament_seeds <- readRDS(here("data", "processed", "tournament_seeds.rds"))

# Fit separate Bradley-Terry models for each season
seasons <- sort(unique(bt_data$season))
all_season_abilities <- list()

for (season_year in seasons) {
    bt_season <- bt_data %>% filter(season == season_year)

    # Fit Bradley-Terry model
    bt_model_season <- BradleyTerry2::BTm(
        outcome = cbind(home.wins, away.wins),
        player1 = home.team,
        player2 = away.team,
        formula = ~ team + home_adv_bar,
        id = "team",
        contrasts = list(team = "contr.sum"),
        data = bt_season
    )

    # Check convergence
    if (!bt_model_season$converged) {
        stop(sprintf(
            "Model FAILED TO CONVERGE for %d season after %d iterations.",
            season_year, bt_model_season$iter
        ))
    }

    abilities_season <- BradleyTerry2::BTabilities(bt_model_season)
    abilities_df <- as.data.frame(abilities_season) %>%
        rownames_to_column(var = "team") %>%
        as_tibble() %>%
        rename(lambda = ability, se = s.e.) %>%
        mutate(season = season_year)

    all_season_abilities[[as.character(season_year)]] <- abilities_df
}

# Combine all seasons
all_abilities <- bind_rows(all_season_abilities)

# Z-score within season
all_abilities_scaled <- all_abilities %>%
    group_by(season) %>%
    mutate(
        season_sd = sd(lambda, na.rm = TRUE),
        season_mean = mean(lambda, na.rm = TRUE),
        lambda_z = (lambda - season_mean) / season_sd,
        se_z = se / season_sd,
        se_z = pmax(se_z, 1e-6)
    ) %>%
    ungroup()

team_abilities_df <- all_abilities_scaled %>%
    group_by(team) %>%
    summarise(
        lambda = weighted.mean(lambda_z, w = 1 / (se_z^2), na.rm = TRUE),
        se = sqrt(1 / sum(1 / (se_z^2), na.rm = TRUE)),
        n_seasons = n(),
        seasons_played = paste(season, collapse = ", "),
        .groups = "drop"
    ) %>%
    arrange(desc(lambda))

# Compute win probability matrix 
lambda_vec <- setNames(team_abilities_df$lambda, team_abilities_df$team)
lambda_diff <- outer(X = lambda_vec, Y = lambda_vec, FUN = "-")
win_probs <- 1 / (1 + exp(-1 * lambda_diff))
win_probs <- pmin(pmax(win_probs, 1e-6), 1 - 1e-6)
diag(win_probs) <- NA

# Add tournament seeding
team_abilities_with_seeds <- team_abilities_df %>%
    left_join(
        tournament_seeds %>% select(team, seed, region),
        by = "team"
    ) %>%
    mutate(
        is_mid_tier_seed = !is.na(seed) & seed >= 8 & seed <= 12,
        seed_category = case_when(
            is.na(seed) ~ "Non-tournament",
            seed <= 4 ~ "1-4 seeds",
            seed <= 7 ~ "5-7 seeds",
            seed <= 12 ~ "8-12 seeds",
            TRUE ~ "13-16 seeds"
        )
    )

# Summary Stats
seed_summary <- team_abilities_with_seeds %>%
    filter(!is.na(seed)) %>%
    group_by(seed_category) %>%
    summarise(
        n_teams = n(),
        mean_lambda = mean(lambda),
        median_lambda = median(lambda),
        sd_lambda = sd(lambda),
        .groups = "drop"
    ) %>%
    arrange(desc(mean_lambda))

mid_tier <- team_abilities_with_seeds %>%
    filter(is_mid_tier_seed) %>%
    select(team, seed, lambda, se) %>%
    arrange(seed, desc(lambda))

# Save results
bt_model <- bt_model_season

saveRDS(bt_model, here("data", "processed", "bt_model.rds"))
saveRDS(all_abilities, here("data", "processed", "team_abilities_by_season.rds"))
saveRDS(team_abilities_df, here("data", "processed", "team_abilities.rds"))
saveRDS(team_abilities_with_seeds, here("data", "processed", "team_abilities_with_seeds.rds"))
saveRDS(win_probs, here("data", "processed", "win_probability_matrix.rds"))

write_csv(team_abilities_df, here("data", "processed", "team_abilities.csv"))
write_csv(team_abilities_with_seeds, here("data", "processed", "team_abilities_with_seeds.csv"))
```

---

## Analysis Approach

Our analysis begins by fitting each latent team strength parameter lambda for all D1 teams using the regular seasons data. Next, for each tournament matchup, we calculate the win probabilities for each 8-12 seed matchups using:

$$
P(\text{win}) = \frac{1}{1 + e^{-(\lambda_i - \lambda_j)}}
$$

We implement this analytically for all 8-12 seed matchups:

```{r analytical-probabilities-code, eval=FALSE}
# From scripts/03_seed_analysis.R

library(tidyverse)
library(here)

bt_model <- readRDS(here("data", "processed", "bt_model.rds"))
team_abilities <- readRDS(here("data", "processed", "team_abilities_with_seeds.rds"))
win_probs <- readRDS(here("data", "processed", "win_probability_matrix.rds"))
tournament_seeds <- readRDS(here("data", "processed", "tournament_seeds.rds"))

# Identify 8-12 Seed Teams and Their Opponents
mid_tier_teams <- team_abilities %>%
    filter(seed >= 8 & seed <= 12) %>%
    arrange(seed, desc(lambda))

# Calculate First Round Win Probabilities
# Define standard NCAA tournament first-round matchups
# 8 plays 9, 9 plays 8, 10 plays 7, 11 plays 6, 12 plays 5
matchup_map <- tibble(
    seed = c(8, 9, 10, 11, 12),
    opponent_seed = c(9, 8, 7, 6, 5),
    round = "Round of 64"
)

# For each 8-12 seed, calculate probability of beating their first-round opponent
first_round_analysis <- mid_tier_teams %>%
    left_join(matchup_map, by = "seed") %>%
    rowwise() %>%
    mutate(
        opponent_lambda = {
            opponent_team <- team_abilities %>%
                filter(seed == opponent_seed, region == region) %>%
                pull(lambda)
            if (length(opponent_team) > 0) opponent_team[1] else NA_real_
        },
        prob_win_round1 = if_else(
            !is.na(opponent_lambda),
            1 / (1 + exp(-(lambda - opponent_lambda))),
            NA_real_
        )
    ) %>%
    ungroup()

first_round_summary <- first_round_analysis %>%
    group_by(seed) %>%
    summarise(
        n_teams = n(),
        avg_prob_win = mean(prob_win_round1, na.rm = TRUE),
        min_prob_win = min(prob_win_round1, na.rm = TRUE),
        max_prob_win = max(prob_win_round1, na.rm = TRUE),
        p10_prob_win = quantile(prob_win_round1, 0.10, na.rm = TRUE),
        p90_prob_win = quantile(prob_win_round1, 0.90, na.rm = TRUE),
        expected_wins = sum(prob_win_round1, na.rm = TRUE)
    )

# Expected number of 8-12 seeds advancing to Round of 32
expected_advance_r32 <- sum(first_round_summary$expected_wins)

# Calculate Second Round Win Probabilities (Round of 32 -> Sweet 16)
# Second round opponents are typically 1-4 seeds
# 8/9 winner plays 1, 5/12 winner plays 4, etc.
second_round_matchups <- tibble(
    seed = c(8, 9, 10, 11, 12),
    r2_opponent_seed = c(1, 1, 2, 3, 4)
)

second_round_analysis <- first_round_analysis %>%
    left_join(second_round_matchups, by = "seed") %>%
    rowwise() %>%
    mutate(
        # Find second round opponent's ability
        r2_opponent_lambda = {
            opponent_team <- team_abilities %>%
                filter(seed == r2_opponent_seed, region == region) %>%
                pull(lambda)
            if (length(opponent_team) > 0) opponent_team[1] else NA_real_
        },
        # Probability of winning second round (conditional on advancing)
        prob_win_round2_conditional = if_else(
            !is.na(r2_opponent_lambda),
            1 / (1 + exp(-(lambda - r2_opponent_lambda))),
            NA_real_
        ),
        # Probability of reaching Sweet 16 (winning both games)
        prob_reach_sweet16 = prob_win_round1 * prob_win_round2_conditional
    ) %>%
    ungroup()

# Summary by seed
second_round_summary <- second_round_analysis %>%
    group_by(seed) %>%
    summarise(
        n_teams = n(),
        avg_prob_win_r2_conditional = mean(prob_win_round2_conditional, na.rm = TRUE),
        avg_prob_reach_sweet16 = mean(prob_reach_sweet16, na.rm = TRUE),
        expected_in_sweet16 = sum(prob_reach_sweet16, na.rm = TRUE)
    )

# Expected number of 8-12 seeds in Sweet 16
expected_in_sweet16 <- sum(second_round_summary$expected_in_sweet16)

# Calculate Elite 8 and Final 4 Probabilities
# Get average abilities of top seeds by region
top_seed_abilities <- team_abilities %>%
    filter(seed <= 4) %>%
    group_by(region) %>%
    summarise(avg_top_lambda = mean(lambda, na.rm = TRUE))

# Calculate probabilities for deeper runs
deeper_runs <- second_round_analysis %>%
    left_join(
        top_seed_abilities %>% select(region, avg_top_lambda),
        by = "region"
    ) %>%
    mutate(
        # Sweet 16 opponent (typically 4/5 or 1 seed)
        prob_win_sweet16 = 1 / (1 + exp(-(lambda - avg_top_lambda))),
        prob_reach_elite8 = prob_reach_sweet16 * prob_win_sweet16,
        # Elite 8 opponent (typically 1 or 2 seed)
        prob_win_elite8 = 1 / (1 + exp(-(lambda - avg_top_lambda - 0.5))),
        prob_reach_final4 = prob_reach_elite8 * prob_win_elite8,
        # Final 4 (facing another region's top team)
        prob_win_final4 = 1 / (1 + exp(-(lambda - avg_top_lambda - 0.5))),
        prob_reach_finals = prob_reach_final4 * prob_win_final4,
        # Championship game
        prob_win_championship = 1 / (1 + exp(-(lambda - avg_top_lambda - 0.5))),
        prob_win_title = prob_reach_finals * prob_win_championship
    )

# Conditional Probabilities (Given Advancement Past Round 2)
# Calculate conditional probabilities
conditional_probs <- deeper_runs %>%
    mutate(
        # P(Elite 8 | Sweet 16)
        prob_elite8_given_sweet16 = prob_reach_elite8 / prob_reach_sweet16,
        # P(Final 4 | Sweet 16)
        prob_final4_given_sweet16 = prob_reach_final4 / prob_reach_sweet16,
        # P(Finals | Sweet 16)
        prob_finals_given_sweet16 = prob_reach_finals / prob_reach_sweet16,
        # P(Champion | Sweet 16)
        prob_champion_given_sweet16 = prob_win_title / prob_reach_sweet16
    )

conditional_summary <- conditional_probs %>%
    group_by(seed) %>%
    summarise(
        n_teams = n(),
        avg_prob_elite8_given_s16 = mean(prob_elite8_given_sweet16, na.rm = TRUE),
        avg_prob_final4_given_s16 = mean(prob_final4_given_sweet16, na.rm = TRUE),
        avg_prob_finals_given_s16 = mean(prob_finals_given_sweet16, na.rm = TRUE),
        avg_prob_champion_given_s16 = mean(prob_champion_given_sweet16, na.rm = TRUE)
    )


deeper_summary <- deeper_runs %>%
    group_by(seed) %>%
    summarise(
        expected_elite8 = sum(prob_reach_elite8, na.rm = TRUE),
        expected_final4 = sum(prob_reach_final4, na.rm = TRUE),
        expected_finals = sum(prob_reach_finals, na.rm = TRUE),
        expected_champions = sum(prob_win_title, na.rm = TRUE)
    )

# Overall averages for 8-12 seeds
overall_conditional <- conditional_probs %>%
    summarise(
        prob_elite8_given_s16 = weighted.mean(prob_elite8_given_sweet16,
            prob_reach_sweet16,
            na.rm = TRUE
        ),
        prob_final4_given_s16 = weighted.mean(prob_final4_given_sweet16,
            prob_reach_sweet16,
            na.rm = TRUE
        ),
        prob_finals_given_s16 = weighted.mean(prob_finals_given_sweet16,
            prob_reach_sweet16,
            na.rm = TRUE
        ),
        prob_champion_given_s16 = weighted.mean(prob_champion_given_sweet16,
            prob_reach_sweet16,
            na.rm = TRUE
        )
    )

saveRDS(first_round_analysis,
    file = here("results", "tables", "first_round_analysis.rds")
)
saveRDS(second_round_analysis,
    file = here("results", "tables", "second_round_analysis.rds")
)
saveRDS(deeper_runs,
    file = here("results", "tables", "deeper_runs_analysis.rds")
)
saveRDS(conditional_probs,
    file = here("results", "tables", "conditional_probabilities.rds")
)

write_csv(first_round_summary,
    file = here("results", "tables", "first_round_summary.csv")
)
write_csv(second_round_summary,
    file = here("results", "tables", "second_round_summary.csv")
)
write_csv(deeper_summary,
    file = here("results", "tables", "deeper_runs_summary.csv")
)
write_csv(conditional_summary,
    file = here("results", "tables", "conditional_summary.csv")
)

key_results <- list(
    expected_advance_r32 = expected_advance_r32,
    expected_in_sweet16 = expected_in_sweet16,
    overall_conditional = overall_conditional,
    first_round_summary = first_round_summary,
    second_round_summary = second_round_summary,
    deeper_summary = deeper_summary,
    conditional_summary = conditional_summary
)
saveRDS(key_results, file = here("results", "tables", "key_results.rds"))
```

Output: Seed-level summary statistics and conditional probabilities

Then, we simulate the 64-teams single elimination March Madness tournaments 5,000 times using Bradley-Terry win probabilities:

```{r simulation-code, eval=FALSE}
# From scripts/04_tournament_simulation.R

library(tidyverse)
library(here)

team_abilities <- readRDS(here("data", "processed", "team_abilities_with_seeds.rds"))
win_probs <- readRDS(here("data", "processed", "win_probability_matrix.rds"))

regions <- sort(unique(team_abilities$region))
if (length(regions) != 4) message("Note: found ", length(regions), " regions; using whatever is present.")

lambda_mat <- matrix(NA_real_,
    nrow = length(regions), ncol = 16,
    dimnames = list(regions, as.character(1:16))
)
team_mat <- matrix(NA_character_,
    nrow = length(regions), ncol = 16,
    dimnames = list(regions, as.character(1:16))
)

tmp <- team_abilities %>%
    select(region, seed, team, lambda) %>%
    arrange(region, seed)

for (i in seq_len(nrow(tmp))) {
    rr <- as.character(tmp$region[i])
    ss <- as.character(tmp$seed[i])
    if (ss %in% colnames(lambda_mat) && rr %in% rownames(lambda_mat) && is.na(lambda_mat[rr, ss])) {
        lambda_mat[rr, ss] <- tmp$lambda[i]
        team_mat[rr, ss] <- tmp$team[i]
    }
}

seed_means <- tapply(tmp$lambda, tmp$seed, mean, na.rm = TRUE)
for (rr in rownames(lambda_mat)) {
    for (ss in colnames(lambda_mat)) {
        if (is.na(lambda_mat[rr, ss])) {
            lambda_mat[rr, ss] <- seed_means[[ss]]
            team_mat[rr, ss] <- paste0("Seed_", ss, "_", rr)
        }
    }
}

get_lambda_fast <- function(seed, region) lambda_mat[region, as.character(seed)]
get_team_fast <- function(seed, region) team_mat[region, as.character(seed)]
inv_logit <- function(x) 1 / (1 + exp(-x))

# Simulate single game
simulate_game <- function(team1_lambda, team2_lambda) {
    p <- inv_logit(team1_lambda - team2_lambda)
    p <- pmin(pmax(p, 1e-6), 1 - 1e-6)
    rbinom(length(p), size = 1, prob = p)
}

# Simulate an entire tournament for ONE simulation, using only base vectors
simulate_tournament_fast <- function(seed_val = NULL) {
    if (!is.null(seed_val)) set.seed(seed_val)

    hi <- c(1, 2, 3, 4, 5, 6, 7, 8)
    lo <- c(16, 15, 14, 13, 12, 11, 10, 9)

    out_rows <- list()

    # Do each region independently
    for (rr in rownames(lambda_mat)) {
        team1_l <- get_lambda_fast(hi, rr)
        team2_l <- get_lambda_fast(lo, rr)
        team1_n <- get_team_fast(hi, rr)
        team2_n <- get_team_fast(lo, rr)

        win1 <- simulate_game(team1_l, team2_l)
        r64_w_seed <- ifelse(win1 == 1, hi, lo)
        r64_w_l <- ifelse(win1 == 1, team1_l, team2_l)
        r64_w_n <- ifelse(win1 == 1, team1_n, team2_n)

        out_rows[[length(out_rows) + 1]] <- tibble(
            round = "Round of 64", region = rr, matchup_id = seq_along(hi),
            team1_seed = hi, team2_seed = lo,
            winner_seed = r64_w_seed, winner_lambda = r64_w_l, winner_name = r64_w_n
        )

        idx32 <- matrix(r64_w_seed, nrow = 2, byrow = TRUE)
        lam32 <- matrix(r64_w_l, nrow = 2, byrow = TRUE)
        nam32 <- matrix(r64_w_n, nrow = 2, byrow = TRUE)

        win2 <- simulate_game(lam32[1, ], lam32[2, ])
        r32_w_seed <- ifelse(win2 == 1, idx32[1, ], idx32[2, ])
        r32_w_l <- ifelse(win2 == 1, lam32[1, ], lam32[2, ])
        r32_w_n <- ifelse(win2 == 1, nam32[1, ], nam32[2, ])

        out_rows[[length(out_rows) + 1]] <- tibble(
            round = "Round of 32", region = rr, matchup_id = 1:4,
            team1_seed = idx32[1, ], team2_seed = idx32[2, ],
            winner_seed = r32_w_seed, winner_lambda = r32_w_l, winner_name = r32_w_n
        )

        idx16 <- matrix(r32_w_seed, nrow = 2, byrow = TRUE)
        lam16 <- matrix(r32_w_l, nrow = 2, byrow = TRUE)
        nam16 <- matrix(r32_w_n, nrow = 2, byrow = TRUE)

        win3 <- simulate_game(lam16[1, ], lam16[2, ])
        r16_w_seed <- ifelse(win3 == 1, idx16[1, ], idx16[2, ])
        r16_w_l <- ifelse(win3 == 1, lam16[1, ], lam16[2, ])
        r16_w_n <- ifelse(win3 == 1, nam16[1, ], nam16[2, ])

        out_rows[[length(out_rows) + 1]] <- tibble(
            round = "Sweet 16", region = rr, matchup_id = 1:2,
            team1_seed = idx16[1, ], team2_seed = idx16[2, ],
            winner_seed = r16_w_seed, winner_lambda = r16_w_l, winner_name = r16_w_n
        )

        idx8 <- c(r16_w_seed[1], r16_w_seed[2])
        lam8 <- c(r16_w_l[1], r16_w_l[2])
        nam8 <- c(r16_w_n[1], r16_w_n[2])

        win4 <- simulate_game(lam8[1], lam8[2])
        e8_w_seed <- ifelse(win4 == 1, idx8[1], idx8[2])
        e8_w_l <- ifelse(win4 == 1, lam8[1], lam8[2])
        e8_w_n <- ifelse(win4 == 1, nam8[1], nam8[2])

        out_rows[[length(out_rows) + 1]] <- tibble(
            round = "Elite 8", region = rr, matchup_id = 1L,
            team1_seed = idx8[1], team2_seed = idx8[2],
            winner_seed = e8_w_seed, winner_lambda = e8_w_l, winner_name = e8_w_n
        )
    }

    # Final 4 (National semis) — pair region winners [1vs2], [3vs4]
    e8 <- bind_rows(out_rows) %>%
        filter(round == "Elite 8") %>%
        arrange(region)
    semi_pairs <- split(e8, rep(1:2, each = 2)) # 2 games

    ff_rows <- lapply(seq_along(semi_pairs), function(k) {
        g <- semi_pairs[[k]]
        lam <- g$winner_lambda
        nam <- g$winner_name
        sed <- g$winner_seed
        win <- simulate_game(lam[1], lam[2])
        tibble(
            round = "Final 4", region = "National", matchup_id = k,
            team1_seed = sed[1], team2_seed = sed[2],
            winner_seed = ifelse(win == 1, sed[1], sed[2]),
            winner_lambda = ifelse(win == 1, lam[1], lam[2]),
            winner_name = ifelse(win == 1, nam[1], nam[2])
        )
    })

    # Championship
    ff <- bind_rows(ff_rows) %>% arrange(matchup_id)
    lamc <- ff$winner_lambda
    namc <- ff$winner_name
    sedc <- ff$winner_seed
    wfin <- simulate_game(lamc[1], lamc[2])
    champ <- tibble(
        round = "Championship", region = "National", matchup_id = 1L,
        team1_seed = sedc[1], team2_seed = sedc[2],
        winner_seed = ifelse(wfin == 1, sedc[1], sedc[2]),
        winner_lambda = ifelse(wfin == 1, lamc[1], lamc[2]),
        winner_name = ifelse(wfin == 1, namc[1], namc[2])
    )

    bind_rows(out_rows, ff_rows, list(champ)) %>% bind_rows()
}

# Run MC Simulations
library(parallel)

N_SIMS <- 5000
N_CORES <- tryCatch(
    {
        cores <- detectCores()
        if (is.na(cores) || cores < 1) 1 else max(1, cores - 1)
    },
    error = function(e) 1
)

simulation_results <- mclapply(
    X = 1:N_SIMS,
    FUN = function(sim) {
        simulate_tournament_fast(seed_val = 479 + sim) %>%
            mutate(sim_id = sim)
    },
    mc.cores = N_CORES,
    mc.preschedule = TRUE
)

all_simulations <- bind_rows(simulation_results)

rm(simulation_results)
gc(verbose = FALSE)

round_order <- c(
    "Round of 64", "Round of 32", "Sweet 16",
    "Elite 8", "Final 4", "Championship"
)

# Count 8-12 seeds in each round for each simulation
mid_tier_counts <- all_simulations %>%
    mutate(round = factor(round, levels = round_order)) %>%
    group_by(sim_id, round) %>%
    summarise(n_mid_tier = sum(winner_seed >= 8 & winner_seed <= 12), .groups = "drop") %>%
    tidyr::complete(
        sim_id,
        round = factor(round_order, levels = round_order),
        fill = list(n_mid_tier = 0L)
    )

# Summary stats by round
mid_tier_summary <- mid_tier_counts %>%
    group_by(round) %>%
    summarise(
        mean_count = mean(n_mid_tier),
        median_count = median(n_mid_tier),
        sd_count = sd(n_mid_tier),
        min_count = min(n_mid_tier),
        max_count = max(n_mid_tier),
        q25 = quantile(n_mid_tier, 0.25),
        q75 = quantile(n_mid_tier, 0.75)
    )

# Championship winners
championship_winners <- all_simulations %>%
    filter(round == "Championship") %>%
    select(sim_id, winner_seed, winner_name)

mid_tier_championships <- championship_winners %>%
    filter(winner_seed >= 8 & winner_seed <= 12)

# Analyze Performance by Individual Seed
seed_performance <- all_simulations %>%
    filter(winner_seed >= 8 & winner_seed <= 12) %>%
    mutate(round = factor(round, levels = round_order)) %>%
    group_by(winner_seed, round) %>%
    summarise(n_reached = n(), .groups = "drop") %>%
    mutate(
        pct_of_sims = (n_reached / N_SIMS) * 100,
        expected_per_seed = n_reached / (N_SIMS * 4)
    )

# Save Simulation Results
saveRDS(all_simulations, file = here("results", "tables", "all_simulations.rds"))

# Save summary statistics
saveRDS(mid_tier_summary, file = here("results", "tables", "simulation_summary.rds"))
write_csv(mid_tier_summary, file = here("results", "tables", "simulation_summary.csv"))

# Save seed-specific performance
saveRDS(seed_performance, file = here("results", "tables", "seed_performance_sim.rds"))
write_csv(seed_performance, file = here("results", "tables", "seed_performance_sim.csv"))

# Save championship winners
write_csv(championship_winners,
    file = here("results", "tables", "championship_winners.csv")
)

# Save mid-tier counts by simulation
write_csv(mid_tier_counts,
    file = here("results", "tables", "mid_tier_counts_by_sim.csv")
)
```


---

# Results

For a tournament, we expect around 2-3 teams (seeds 8-12) to get to the Sweet 16. Our simulations align with this posturization, with an average of 2.21 teams advancing as the tournaments have about 1-3 mid-tier seeds advance. In the 2023-24 season, where no teams in seeds 8-12 advanced, the probability of occurring was about 6.8%.

Teams with great regular-season game performance have better chances to make it to the Sweet 16 despite seed number. Team strength values vary a lot, where the strongest teams in this mid-tier 8-12 seeds have a probability to make it to the Sweet 16 around 2%, while the weaker teams are around 0%.

When a mid-tier team gets into the Sweet 16, there is about a 14% chance of getting into the Final 4 and less than 2% chance to win overall. 

### Team Strength Estimates

```{r team-strength-code, eval=FALSE}
# From scripts/05_visualization.R

library(ggplot2)

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

p1 <- team_abilities %>%
    filter(!is.na(seed)) %>%
    mutate(
        seed_category = factor(
            seed_category,
            levels = c("1-4 seeds", "5-7 seeds", "8-12 seeds", "13-16 seeds")
        )
    ) %>%
    ggplot(aes(x = seed_category, y = lambda, fill = seed_category)) +
    geom_boxplot(width = 0.65, outlier.alpha = 0.35) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray60") +
    scale_fill_manual(values = oi_colors[c(1, 2, 3, 6)]) +
    labs(
        title = "Team Strength Distribution by Seed Category",
        subtitle = "λ re-centered and scaled within season",
        x = "Seed Category",
        y = "Standardized strength (λ, z-score per season)"
    ) +
    theme_minimal() +
    theme(legend.position = "none")

ggsave(here("results", "figures", "01_team_strength_by_seed.png"), 
       p1, width = 10, height = 6, dpi = 400)
```

```{r team-strength-plot, echo=FALSE, fig.cap="Team strength distribution by seed"}
knitr::include_graphics(here("results", "figures", "01_team_strength_by_seed.png"))
```

The Bradley-Terry model indicates that higher seeds tend to have a higher pattern of team strengths. Seeds 1-4 (strong) have a lambda of > 8, whereas seeds 8-12 (mid-tier) have a 4 < lambda < 6. Thus, regular game performance in regard to team strength is strongly correlated with our estimated output from the model. 

### Top 10 Strongest Teams

```{r top-teams, echo=FALSE}
if (!is.null(team_abilities)) {
    top_teams <- team_abilities %>%
        filter(!is.na(seed)) %>%
        arrange(desc(lambda)) %>%
        head(10) %>%
        select(
            Team = team, Seed = seed, Region = region,
            Lambda = lambda, `Std. Error` = se
        ) %>%
        mutate(
            Lambda = sprintf("%.3f", Lambda),
            `Std. Error` = sprintf("%.3f", `Std. Error`)
        )

    kable(top_teams, caption = "Top 10 Strongest Teams with Lambda = log-strength (Bradley-Terry scale)") %>%
        kable_styling(bootstrap_options = c("striped", "hover"))
}
```

The top-10 teams are seeds 1-4, which confirms that the selection committee seed decisions align with regular-season strength. The standard errors (SEs) are small, indicating our precise strength estimates.

### Within-Seed Variation & Upset Candidates

```{r upset-potential-code, eval=FALSE}
# From scripts/05_visualization.R

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

p10 <- team_abilities %>%
    filter(seed >= 8, seed <= 12) %>%
    mutate(seed = factor(seed)) %>%
    ggplot(aes(x = seed, y = lambda, color = seed)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    geom_jitter(width = 0.2, alpha = 0.6, size = 3) +
    stat_summary(
        fun = median, geom = "crossbar",
        width = 0.5, color = "black", linewidth = 0.8
    ) +
    scale_color_manual(values = oi_colors) +
    labs(
        title = "Team Strength Variability Within Seeds 8-12",
        subtitle = "Each point represents a team; crossbar shows median",
        x = "Seed", y = "Standardized Strength (λ)"
    ) +
    theme_minimal() +
    theme(legend.position = "none")

ggsave(here("results", "figures", "10_upset_potential.png"), 
       p10, width = 10, height = 6, dpi = 400)
```

```{r upset-potential-plot, echo=FALSE, fig.cap="Team strength Variability Within the 8-12 seeds."}
knitr::include_graphics(here("results", "figures", "10_upset_potential.png"))
```

Not all teams with the same seed are equally strong. Understanding this variability is important to help us identify upset candidates. The figure shows that seed 11 has high variability, meaning some 11-seeds are much stronger than average, while others are weaker. For example, the strongest in seed 11 is the Iowa Hawkeyes, which has a lambda value of 0.59. However, the weakest is the Maine Black Bears with a lambda value near zero. This demonstrates why tournament outcomes can vary drastically for teams within the same seed.

### First Round Performance (8-12 Seeds)

```{r first-round-code, eval=FALSE}
# From scripts/05_visualization.R

all_simulations <- readRDS(here("results", "tables", "all_simulations.rds"))

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

# Calculate first round probabilities from simulation results
first_round_by_sim <- all_simulations %>%
    filter(round == "Round of 64") %>%
    mutate(
        seed = case_when(
            winner_seed == 8 ~ 8L,
            winner_seed == 9 ~ 9L,
            winner_seed == 10 ~ 10L,
            winner_seed == 11 ~ 11L,
            winner_seed == 12 ~ 12L,
            TRUE ~ NA_integer_
        )
    ) %>%
    filter(!is.na(seed)) %>%
    count(sim_id, seed, name = "wins") %>%
    tidyr::complete(sim_id, seed = 8:12, fill = list(wins = 0L)) %>%
    mutate(wins = replace_na(wins, 0L))

first_round_ci <- first_round_by_sim %>%
    group_by(seed) %>%
    summarise(
        mean_wins = mean(wins),
        lo = quantile(wins, 0.025),
        hi = quantile(wins, 0.975),
        .groups = "drop"
    ) %>%
    mutate(
        mean_prob = mean_wins / 4,
        lo_prob = lo / 4,
        hi_prob = hi / 4
    )

p3 <- first_round_ci %>%
    ggplot(aes(x = factor(seed), y = mean_prob)) +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray60", linewidth = 0.5) +
    geom_col(fill = oi_colors[3], alpha = 0.85) +
    geom_errorbar(
        aes(ymin = lo_prob, ymax = hi_prob),
        width = 0.3, color = oi_colors[5], linewidth = 0.8
    ) +
    geom_text(
        aes(label = scales::percent(mean_prob, accuracy = 0.1)),
        vjust = -0.5, size = 4, fontface = "bold"
    ) +
    scale_y_continuous(
        labels = scales::percent_format(),
        limits = c(0, 1),
        expand = expansion(mult = c(0, 0.1))
    ) +
    labs(
        title = "First Round Win Probability by Seed (8–12)",
        subtitle = "Per-team probability of advancing to Round of 32 (from 5,000 simulations)",
        x = "Seed", y = "Win Probability"
    ) +
    theme_minimal()

ggsave(here("results", "figures", "03_first_round_probabilities.png"), 
       p3, width = 10, height = 6, dpi = 400)
```

```{r first-round-plot, echo=FALSE, fig.cap="First round Win Probabilities for 8-12 seeds"}
knitr::include_graphics(here("results", "figures", "03_first_round_probabilities.png"))
```

**Note on methodology**: The figure above displays simulation-based probabilities (averaged across 5,000 full tournament simulations), while the table below shows analytical probabilities (calculated directly from Bradley-Terry model matchup predictions). The simulation approach accounts for the full tournament structure and variability across many iterations, while the analytical approach provides direct pairwise win probabilities. Both methods are valid and complement each other—the small differences between them reflect the inherent uncertainty in tournament outcomes.

```{r first-round-table, echo=FALSE}
if (!is.null(key_results) && !is.null(key_results$first_round_summary)) {
    first_round_summary <- key_results$first_round_summary %>%
        select(
            Seed = seed, `N Teams` = n_teams,
            `Avg Win Prob` = avg_prob_win, `Expected Wins` = expected_wins
        ) %>%
        mutate(
            `Avg Win Prob` = sprintf("%.1f%%", `Avg Win Prob` * 100),
            `Expected Wins` = sprintf("%.2f", `Expected Wins`)
        )

    kable(first_round_summary,
        caption = "Analytical First Round Win Probabilities by Seed"
    ) %>%
        kable_styling(bootstrap_options = c("striped", "hover"))
}
```

We find that for an almost even match:

- 8 seeds versus 9 seeds: 49% Win Probability
- 9 seeds versus 8 seeds: 51% Win Probability

These close win probabilities show that 8 seeds versus 9 seeds have unpredictable first-round games.

However, for an uneven match:

- 10 seeds versus 7 seeds: 34% Win Probability
- 12 seeds versus 5 seeds: 25% Win Probability

Therefore, we would expect approximately 8-9 from the 20 teams in the 8-12 seeds would win their first-round game and advance.

### Advancement to the Sweet 16

```{r analytical-vs-sim-code, eval=FALSE}
# From scripts/05_visualization.R

mid_tier_counts <- read_csv(here("results", "tables", "mid_tier_counts_by_sim.csv"), 
                            show_col_types = FALSE)
mid_tier_summary <- readRDS(here("results", "tables", "simulation_summary.rds"))
first_round_summary <- read_csv(here("results", "tables", "first_round_summary.csv"), 
                                show_col_types = FALSE)
second_round_summary <- read_csv(here("results", "tables", "second_round_summary.csv"), 
                                 show_col_types = FALSE)
deeper_summary <- read_csv(here("results", "tables", "deeper_runs_summary.csv"), 
                          show_col_types = FALSE)

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

# Calculate confidence intervals from simulations
summary_ci <- mid_tier_counts %>%
    filter(round %in% c("Round of 32", "Sweet 16", "Elite 8", "Final Four")) %>%
    group_by(round) %>%
    summarise(
        mean = mean(n_mid_tier, na.rm = TRUE),
        lo = quantile(n_mid_tier, 0.025, na.rm = TRUE),
        hi = quantile(n_mid_tier, 0.975, na.rm = TRUE),
        .groups = "drop"
    )

# Compare analytical vs simulation results
comparison_data <- tibble(
    round = c("Round of 32", "Sweet 16", "Elite 8", "Final Four"),
    analytical = c(
        sum(first_round_summary$expected_wins),
        sum(second_round_summary$expected_in_sweet16),
        sum(deeper_summary$expected_elite8),
        sum(deeper_summary$expected_final4)
    )
) %>%
    left_join(
        mid_tier_summary %>%
            filter(round %in% c("Round of 32", "Sweet 16", "Elite 8", "Final Four")) %>%
            select(round, simulated = mean_count, sim_sd = sd_count),
        by = "round"
    ) %>%
    left_join(summary_ci, by = "round") %>%
    pivot_longer(
        cols = c(analytical, simulated),
        names_to = "method",
        values_to = "expected_count"
    ) %>%
    mutate(
        round = factor(round, levels = c("Round of 32", "Sweet 16", "Elite 8", "Final Four")),
        method = factor(method,
            levels = c("analytical", "simulated"),
            labels = c("Analytical Model", "Monte Carlo Simulation")
        )
    )

# Create comparison plot
p6 <- comparison_data %>%
    ggplot(aes(x = round, y = expected_count, fill = method)) +
    geom_col(position = position_dodge(width = 0.8), alpha = 0.8) +
    geom_errorbar(
        data = comparison_data %>% filter(method == "Monte Carlo Simulation"),
        aes(ymin = lo, ymax = hi),
        position = position_dodge(width = 0.8),
        width = 0.25,
        color = "gray30"
    ) +
    geom_text(
        aes(label = sprintf("%.2f", expected_count)),
        position = position_dodge(width = 0.8),
        vjust = -0.5,
        size = 3.5
    ) +
    scale_fill_manual(
        values = c(
            "Analytical Model" = oi_colors[2],
            "Monte Carlo Simulation" = oi_colors[6]
        )
    ) +
    scale_y_continuous(
        trans = "sqrt",
        breaks = c(0, 1, 2, 3, 5, 7, 10),
        limits = c(0, NA),
        expand = expansion(mult = c(0, 0.15))
    ) +
    labs(
        title = "Expected 8-12 Seeds Advancing: Model vs Simulation",
        subtitle = "Monte Carlo (5,000 sims)",
        x = "Tournament Round",
        y = "Expected Number of 8-12 Seeds",
        fill = "Method"
    ) +
    theme_minimal() +
    theme(
        plot.title = element_text(face = "bold", size = 14),
        axis.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 15, hjust = 1),
        legend.position = "bottom"
    )

ggsave(here("results", "figures", "06_analytical_vs_simulation.png"),
       p6, width = 12, height = 7, dpi = 400)
```

```{r analytical-vs-sim, echo=FALSE, fig.cap="Analytical predictions versus Monte Carlo simulation results"}
knitr::include_graphics(here("results", "figures", "06_analytical_vs_simulation.png"))
```

Approximately 2.21 of the 8-12 seeds (from 5,000 simulations) are estimated to advance to the Sweet 16 in a tournament.

```{r advancement-table, echo=FALSE}
if (!is.null(key_results) && !is.null(key_results$second_round_summary)) {
    advancement <- key_results$second_round_summary %>%
        select(
            Seed = seed, `N Teams` = n_teams,
            `P(Round 2 Win given Round 1 Win)` = avg_prob_win_r2_conditional,
            `P(Sweet 16)` = avg_prob_reach_sweet16,
            `Expected in Sweet 16` = expected_in_sweet16
        ) %>%
        mutate(
            `P(Round 2 Win given Round 1 Win)` = sprintf(
                "%.1f%%",
                `P(Round 2 Win given Round 1 Win)` * 100
            ),
            `P(Sweet 16)` = sprintf("%.1f%%", `P(Sweet 16)` * 100),
            `Expected in Sweet 16` = sprintf("%.2f", `Expected in Sweet 16`)
        )

    kable(advancement,
        caption = "Sweet 16 Advancement Probabilities by Seed"
    ) %>%
        kable_styling(bootstrap_options = c("striped", "hover"))
}
```

Therefore, if an 8 or 9 seed wins Round 1, they would likely go on to face seeds 1 or 2 in Round 2, which lowers the Sweet 16 probability by a lot. 

Specifically, seeds 10-12 face would likely face seeds 3-5 in Round 2, so if they advance, it is difficult, but better chances than seeds 8-9 since they have to face seeds 1-2. 

### Further Advancement Beyond the Sweet 16

```{r conditional-probabilities-code, eval=FALSE}
# From scripts/05_visualization.R

conditional_summary <- read_csv(here("results", "tables", "conditional_summary.csv"), 
                                show_col_types = FALSE)

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

cond_long <- conditional_summary %>%
    select(seed, Elite_8 = avg_prob_elite8_given_s16, 
           `Final Four` = avg_prob_final4_given_s16,
           Championship = avg_prob_finals_given_s16,
           `Win Title` = avg_prob_champion_given_s16) %>%
    pivot_longer(-seed, names_to = "Round", values_to = "Probability")

p7 <- cond_long %>%
    mutate(
        Round = factor(Round, levels = c("Elite_8", "Final Four", "Championship", "Win Title")),
        seed = factor(seed)
    ) %>%
    ggplot(aes(x = seed, y = Probability, fill = seed)) +
    geom_col(alpha = 0.9, width = 0.75) +
    geom_text(
        aes(label = scales::percent(Probability, accuracy = 0.1)),
        vjust = -0.4, size = 3.5, fontface = "bold"
    ) +
    facet_wrap(~Round, ncol = 2, scales = "free_y") +
    scale_y_continuous(labels = scales::percent_format(), 
                       expand = expansion(mult = c(0, 0.15))) +
    scale_fill_manual(values = oi_colors) +
    labs(
        title = "Conditional Advancement Probabilities for 8–12 Seeds",
        subtitle = "Given a team reaches the Sweet 16",
        x = "Seed", y = "Probability"
    ) +
    theme_minimal() +
    theme(legend.position = "none")

ggsave(here("results", "figures", "07_conditional_probabilities.png"), 
       p7, width = 10, height = 6, dpi = 400)
```

```{r conditional-plot, echo=FALSE, fig.cap="Conditional probabilities given Sweet 16 outcome"}
knitr::include_graphics(here("results", "figures", "07_conditional_probabilities.png"))
```

```{r conditional-table, echo=FALSE}
if (!is.null(key_results) && !is.null(key_results$conditional_summary)) {
    conditional <- key_results$conditional_summary %>%
        select(
            Seed = seed,
            `Elite 8` = avg_prob_elite8_given_s16,
            `Final 4` = avg_prob_final4_given_s16,
            `Finals` = avg_prob_finals_given_s16,
            `Champion` = avg_prob_champion_given_s16
        ) %>%
        mutate(
            Seed = as.integer(Seed),
            across(-Seed, ~ sprintf("%.1f%%", . * 100))
        )

    kable(conditional,
        caption = "Conditional Advancement Probabilities by Seed (Given Sweet 16 Appearance)",
        align = c("c", "c", "c", "c", "c")
    ) %>%
        kable_styling(bootstrap_options = c("striped", "hover")) %>%
        add_header_above(c(" " = 1, "Probability of Reaching (given Sweet 16)" = 4))
}
```

When a seed 8-12 moves onto the Sweet 16, they have already overcome the obstacle of beating better opponents. Notably, given that a 8-12 seed team reaches the Swett 16, there is a 45% chance of reaching the Elite 8, which means that they are still good competitors once they proved themselves in the Sweet 16. However, there is still a low probability of winning, as 1.9% of teams that achieve the Sweet 16 will win the championship. There are conditional probabilities differ between seeds as seeds 8-9 are stronger by a little compared to seeds 10-12. 

### Monte Carlo Simulation

```{r simulation-distributions-code, eval=FALSE}
# From scripts/05_visualization.R

mid_tier_counts <- read_csv(here("results", "tables", "mid_tier_counts_by_sim.csv"), 
                            show_col_types = FALSE)

oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
               "#0072B2", "#D55E00", "#CC79A7", "#999999")

round_order <- c("Round of 64", "Round of 32", "Sweet 16", 
                 "Elite 8", "Final Four", "Championship")

p5 <- mid_tier_counts %>%
    filter(round != "Round of 64") %>%
    mutate(round = factor(round, levels = round_order)) %>%
    ggplot(aes(x = n_mid_tier, fill = round)) +
    geom_histogram(binwidth = 1, color = "white", linewidth = 0.3, alpha = 0.9) +
    facet_wrap(~round, ncol = 2, scales = "free_x") +
    scale_fill_manual(values = rep(oi_colors, length.out = 6)) +
    labs(
        title = "Distribution of 8-12 Seed Counts Across Tournament Rounds",
        subtitle = "From 5,000 tournament simulations",
        x = "Number of 8-12 Seeds Advancing",
        y = "Number of Simulations"
    ) +
    theme_minimal() +
    theme(legend.position = "none")

ggsave(here("results", "figures", "05_simulation_distributions.png"), 
       p5, width = 10, height = 6, dpi = 400)
```

```{r simulation-plot, echo=FALSE, fig.cap="Distribution of 8-12 Seeds by Round Advancements"}
knitr::include_graphics(here("results", "figures", "05_simulation_distributions.png"))
```

```{r simulation-summary, echo=FALSE}
if (!is.null(key_results) && !is.null(key_results$simulation_summary)) {
    sim_summary <- key_results$simulation_summary %>%
        filter(round %in% c("Sweet 16", "Elite 8", "Final 4", "Championship")) %>%
        select(
            Round = round,
            `Mean Count` = mean_count,
            `Median Count` = median_count,
            `SD` = sd_count,
            `Min` = min_count,
            `Max` = max_count,
            `P(Zero)` = prob_zero
        ) %>%
        mutate(
            `Mean Count` = round(`Mean Count`, 2),
            `SD` = round(`SD`, 2),
            `P(Zero)` = sprintf("%.1f%%", `P(Zero)` * 100)
        )

    kable(sim_summary,
        caption = "8-12 Seed Counts by Round Advancements"
    ) %>%
        kable_styling(bootstrap_options = c("striped", "hover"))
}
```

The Monte Carlo simulations (5,000 replications) align with our analytical predictions. The distributions show substantial variation—some tournaments have no mid-tier seeds in the Sweet 16, while others have 3-4. The distribution progressively narrows as 8-10 of the 20 teams in seeds 8-12 make it to the Round of 32, then 3-4 of those same teams make it to the Sweet 16. From there, 1-2 of the same teams make it to the Elite 8, and about one team makes it to the Final 4.

### Historical Example: 2023-2024

We will now compare our predicted versus actual results to the NCAA outcomes of the tournament in 2023-2024. Our model estimates that 2.21 teams in the seeds 8-12 will advance to the Sweet Sixteen (on average); however, it was observed that no seeds 8-12 advantage to the Sweet Sixteen, given that these were the seeds designated by the WNCAA. Our model stated that the probability of no teams with seeds 8-12 advancing to the Sweet Sixteen was 6.84%, which is quite unusual. Thus, the observed results of 2023-2024 were statistically unusual and uncommon, which may have been due to strong teams in the top seeds, weaker teams in the seeds 8-12, injuries, etc.

---

## Limitations & Improvements

### Data Limitations

The data limitations we encountered were:

- **Neutral game locations**: We assume by the data that the tournament games are at neutral sites, but some teams may be located closer to the locations than others, which creates a temporary home-court advantage for some teams to have more support than others.

- **Excluded NCAA tournament games**: We wanted to avoid fitting the model using tournament performance, as that would overvalidate our results and not predict based on regular-season performance. However, tournament performance may have skills and team strength qualities that were not apparent in regular-season games.

- **Aggregate data across seasons**: Our estimates include data from 5 total seasons (2019 and 2021-2024), which means we show the team's strength across years. Our estimated team strength is represented as an average across multiple years, which does not account for team composition or coach changes. Thus, the estimated results of the 2024 tournament capture simulated data from the prior seasons, rather than just the 2024 season.

- **Seeds based on team strength**: The data does not provide the NCAA's tournament seeds, which are determined by schedule strength, location, quality wins/losses, etc., so our seeds would not match. Thus, our seeds are interpreted as if seeding were to perfectly match team strength (versus observed tournament seeds). As a result, our estimate to the question about "how many 8-12 seeds advance onto the Sweet Sixteen" is decided by which teams have the seed 8-12 team strength level. The interpretation of our results should reflect that these are strength-based estimates if the seeds matched team strength.

### Future Improvements

To improve, we found 4 strategies:

- **Include observed WNCAA brackets**: An improvement for the future is to include the observed WNCAA brackets rather than our strength-based seeds.

- **Weight by opponent strength**: We can weight the team strength estimates in relation to the estimated strength of the opponents; if a team plays a significantly worse team, their outcome reflects the blowout win as less impactful to their team strength estimate.

- **Incorporate player-level data**: To adjust for team-composition changes in the roster, we can incorporate player-level data when they leave or are added to a team.

- **Weight by time**: We can weight by time, as teams usually evolve throughout the season by building stronger bonds and efficient team dynamics.

---

References:

Bradley, R.A. and Terry, M.E. (1952). "Rank analysis of incomplete block designs: I. The method of paired comparisons." Biometrika, 39(3/4), 324-345.

Bulut, O. (2020). Conducting Monte Carlo simulations in R. University of Alberta – Scholars Portal. https://ualberta.scholaris.ca/items/7ae61a5a-8443-42c2-b5f8-a4d40d0bf3b7 

Cameron, E., & Hocking, T. D. (2025, April 10). BradleyTerry2: Bradley-Terry models in R [Vignette]. The Comprehensive R Archive Network. https://cran.r-project.org/web/packages/BradleyTerry2/vignettes/BradleyTerry.html 

Hutchinson, G., Gilani, S., et al. (2023). wehoop: Women's Basketball Data. R package version 1.5.0. https://github.com/sportsdataverse/wehoop

Hyndman, R. J. (2023, November 1). AIC calculations. Hyndsight Blog. https://robjhyndman.com/hyndsight/lm_aic.html 

Nesbitt, S. (2024). wncaahoopR: Women's NCAA Basketball Data Package. https://github.com/snestler/wncaahoopR

Wikipedia contributors. (2025, November 6). Akaike information criterion. Wikipedia. https://en.wikipedia.org/wiki/Akaike_information_criterion 
